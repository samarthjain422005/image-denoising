{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, UpSampling2D, Dropout, Concatenate, Add, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, mean_squared_error as mse, structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define working directory and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work = input('input system path for working directory :')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = input('input system path for test dataset :')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = input('input system path for predicted :')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = work\n",
    "noisy_train_path = os.path.join(working_dir, \"Train/low\")\n",
    "clean_train_path = os.path.join(working_dir, \"Train/high\")\n",
    "noisy_test_path = test_dir\n",
    "predicted_dir = predict\n",
    "\n",
    "# Ensure predicted directory exists\n",
    "os.makedirs(predicted_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a sorted list of images from the subdirectory specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_train_images = sorted(glob.glob(os.path.join(noisy_train_path, '*')))\n",
    "clean_train_images = sorted(glob.glob(os.path.join(clean_train_path, '*')))\n",
    "noisy_test_images = sorted(glob.glob(os.path.join(noisy_test_path, '*')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(path):\n",
    "    img = Image.open(path)\n",
    "    img = img.resize((256, 256))  # Resizing to 256x256\n",
    "    img = img.convert(\"L\")  # Convert to grayscale\n",
    "    img = np.asarray(img, dtype=\"float32\") / 255.0\n",
    "    img = np.reshape(img, (256, 256, 1))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "noised_train = [image_preprocessing(f) for f in noisy_train_images]\n",
    "cleaned_train = [image_preprocessing(f) for f in clean_train_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(noised_train)\n",
    "y_train = np.asarray(cleaned_train)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_filters=64, kernel_size=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = Conv2D(filters=self.num_filters, kernel_size=self.kernel_size, padding='same', activation='relu')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class DWT_downsampling(tf.keras.layers.Layer):\n",
    "    def call(self, x):\n",
    "        x1 = x[:, 0::2, 0::2, :]  # x(2i−1, 2j−1)\n",
    "        x2 = x[:, 1::2, 0::2, :]  # x(2i, 2j-1)\n",
    "        x3 = x[:, 0::2, 1::2, :]  # x(2i−1, 2j)\n",
    "        x4 = x[:, 1::2, 1::2, :]  # x(2i, 2j)\n",
    "\n",
    "        x_LL = x1 + x2 + x3 + x4\n",
    "        x_LH = -x1 - x3 + x2 + x4\n",
    "        x_HL = -x1 + x3 - x2 + x4\n",
    "        x_HH = x1 - x3 - x2 + x4\n",
    "\n",
    "        return Concatenate(axis=-1)([x_LL, x_LH, x_HL, x_HH])\n",
    "\n",
    "class IWT_upsampling(tf.keras.layers.Layer):\n",
    "    def call(self, x):\n",
    "        x_LL = x[:, :, :, 0:x.shape[3] // 4]\n",
    "        x_LH = x[:, :, :, x.shape[3] // 4:x.shape[3] // 4 * 2]\n",
    "        x_HL = x[:, :, :, x.shape[3] // 4 * 2:x.shape[3] // 4 * 3]\n",
    "        x_HH = x[:, :, :, x.shape[3] // 4 * 3:]\n",
    "\n",
    "        x1 = (x_LL - x_LH - x_HL + x_HH) / 4\n",
    "        x2 = (x_LL - x_LH + x_HL - x_HH) / 4\n",
    "        x3 = (x_LL + x_LH - x_HL - x_HH) / 4\n",
    "        x4 = (x_LL + x_LH + x_HL + x_HH) / 4\n",
    "\n",
    "        y1 = tf.stack([x1, x3], axis=2)\n",
    "        y2 = tf.stack([x2, x4], axis=2)\n",
    "        shape = tf.shape(x)\n",
    "        return tf.reshape(tf.concat([y1, y2], axis=-1), tf.stack([shape[0], shape[1] * 2, shape[2] * 2, shape[3] // 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input = Input(shape=(256, 256, 1))\n",
    "    cb_1 = Conv_block(num_filters=64)(input)\n",
    "    dwt_1 = DWT_downsampling()(cb_1)\n",
    "    cb_2 = Conv_block(num_filters=128)(dwt_1)\n",
    "    dwt_2 = DWT_downsampling()(cb_2)\n",
    "    cb_3 = Conv_block(num_filters=256)(dwt_2)\n",
    "    dwt_3 = DWT_downsampling()(cb_3)\n",
    "    cb_4 = Conv_block(num_filters=512)(dwt_3)\n",
    "    dwt_4 = DWT_downsampling()(cb_4)\n",
    "    cb_5 = Conv_block(num_filters=512)(dwt_4)\n",
    "    cb_5 = BatchNormalization()(cb_5)\n",
    "    cb_5 = Conv_block(num_filters=512)(cb_5)\n",
    "    cb_5 = Conv2D(filters=2048, kernel_size=3, strides=1, padding='same')(cb_5)\n",
    "    up = IWT_upsampling()(cb_5)\n",
    "    up = Conv_block(num_filters=512)(Add()([up, cb_4]))\n",
    "    up = Conv2D(filters=1024, kernel_size=3, strides=1, padding='same')(up)\n",
    "    up = IWT_upsampling()(up)\n",
    "    up = Conv_block(num_filters=256)(Add()([up, cb_3]))\n",
    "    up = Conv2D(filters=512, kernel_size=3, strides=1, padding='same')(up)\n",
    "    up = IWT_upsampling()(up)\n",
    "    up = Conv_block(num_filters=128)(Add()([up, cb_2]))\n",
    "    up = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(up)\n",
    "    up = IWT_upsampling()(up)\n",
    "    up = Conv_block(num_filters=64)(Add()([up, cb_1]))\n",
    "    up = Conv2D(filters=128, kernel_size=3, strides=1, padding='same')(up)\n",
    "    out = Conv2D(filters=1, kernel_size=(1, 1), padding=\"same\")(up)\n",
    "    model = Model(inputs=[input], outputs=[out])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_train = len(noisy_train_images) // 16\n",
    "steps_per_epoch_validation = len(noisy_test_images) // 16\n",
    "\n",
    "callbacks_lst = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', min_lr=0.0000009, min_delta=0.0001, factor=0.70, patience=3, verbose=1, mode='min'),\n",
    "    EarlyStopping(monitor='val_loss', mode='min', verbose=1, min_delta=0.0001, patience=10)\n",
    "]\n",
    "\n",
    "model.compile(loss=MeanSquaredError(), optimizer=Adam(learning_rate=0.0009))\n",
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_val, y_val),\n",
    "          steps_per_epoch=steps_per_epoch_train,\n",
    "          validation_steps=steps_per_epoch_validation,\n",
    "          epochs=300,\n",
    "          verbose=1,\n",
    "          callbacks=callbacks_lst)\n",
    "model.save(os.path.join(working_dir, 'model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess test images\n",
    "noised_test = [image_preprocessing(f) for f in noisy_test_images]\n",
    "x_test = np.asarray(noised_test)\n",
    "\n",
    "# Predict denoised images\n",
    "pred = model.predict(x_test, batch_size=16)\n",
    "\n",
    "psnr_values = []\n",
    "mse_values = []\n",
    "ssim_values = []\n",
    "\n",
    "# Ensure predicted directory exists\n",
    "os.makedirs(predicted_dir, exist_ok=True)\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    original = np.clip(x_test[i] * 255.0, 0, 255)  # Clip values to [0, 255]\n",
    "    denoised = np.clip(pred[i] * 255.0, 0, 255)   # Clip values to [0, 255]\n",
    "\n",
    "    psnr_value = psnr(original, denoised, data_range=255)  # Specify data_range\n",
    "    mse_value = mse(original, denoised)\n",
    "    ssim_value = ssim(original, denoised, data_range=255)\n",
    "\n",
    "    psnr_values.append(psnr_value)\n",
    "    mse_values.append(mse_value)\n",
    "    ssim_values.append(ssim_value)\n",
    "\n",
    "    denoised_img = Image.fromarray(denoised.squeeze().astype(np.uint8))\n",
    "    denoised_img.save(os.path.join(predicted_dir, os.path.basename(noisy_test_images[i])))\n",
    "\n",
    "mean_psnr = np.mean(psnr_values)\n",
    "mean_mse = np.mean(mse_values)\n",
    "mean_ssim = np.mean(ssim_values)\n",
    "\n",
    "print(f\"Mean PSNR: {mean_psnr:.2f}\")\n",
    "print(f\"Mean MSE: {mean_mse:.4f}\")\n",
    "print(f\"Mean SSIM: {mean_ssim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some examples with their PSNR values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 25))\n",
    "for i in range(0, 8, 2):\n",
    "    if i >= len(x_test):\n",
    "        break\n",
    "    plt.subplot(4, 2, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(x_test[i][:, :, 0], cmap='gray')\n",
    "    plt.title('Test Image with Noise')\n",
    "\n",
    "    plt.subplot(4, 2, i + 2)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(pred[i][:, :, 0], cmap='gray')\n",
    "    plt.title(f'Denoised by Autoencoder - PSNR: {psnr_values[i]:.2f} dB')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
